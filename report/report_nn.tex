\documentclass{article}

% NIPS style
\usepackage{nips12submit_e, times}

% figures
\usepackage{graphicx}
\usepackage{subfigure} 

% links
\usepackage[hidelinks]{hyperref}

% better tables
\usepackage{booktabs}
\usepackage{multirow}
\newcommand{\ra}[1]{\renewcommand{\arraystretch}{#1}}


\title{Semi-Supervised Recursive Autoencoders}


\author{
Adrian Guthals \\
\texttt{aguthals@cs.ucsd.edu} \\
\And
David Larson \\
\texttt{dplarson@ucsd.edu} \\
}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

% don't show line numbers
\nipsfinalcopy 


\begin{document}

\maketitle


\begin{abstract}
We evaluate semi-supervised recursive autoencoders (RAE) as a method for predicting the sentiment of sentences. Using random word initialization, we are able to predict the sentiment of a movie review dataset with a 00.0\% accuracy, which is comparable to the 00.0\% accuracy reported in the 2011 paper ``Semi-Supervised Recursive Autoencoders'' by Soch et al.
\end{abstract}



%-----------------------------------------------------------------------------
% INTRO
%-----------------------------------------------------------------------------
\section{Introduction}

Socher et al. presented a semi-supervised method for learning meanings of sentences using recursive autoencoders \cite{Socher}.

The lecture notes state blah \cite{CSE250B}.

Mention: neural networks, sentence meaning/sentiment



%-----------------------------------------------------------------------------
% ALGORITHMS
%-----------------------------------------------------------------------------
\section{Recursive Autoencoders}
RAE, neural networks, backpropogation, error functions, greedy algorithm, calculating derivatives numerically using finite center-difference


\subsection{Error Function}
\begin{equation}
    E_1 (k) =
\end{equation}

\begin{equation}
    E_2 (k) =
\end{equation}


\subsection{Binary Tree Construction}

\subsection{Goal of Training}


\subsection{Gradient Verification}
It is important to verify the accuracy of the gradients calculated using backpropogation. For this study we have chosen to verify the accuracy of backpropogation by comparing against gradients calculated numerically using finite central-differences:
\begin{equation}
    \frac{\partial J}{\partial \theta} = \frac{J(\theta + \epsilon) - J(\theta - \epsilon)}{2\epsilon} + O(\epsilon ^2)
\end{equation}
where $\epsilon$ is the grid spacing.



%-----------------------------------------------------------------------------
% EXPERIMENTS
%-----------------------------------------------------------------------------
\section{Experiments}

%
% DATASETS
%
\subsection{Datasets}
We use the same movie reviews dataset as in \cite{Socher}, which consists of 10662 snippets from reviews posted to the Rotten Tomatoes website\footnote{\url{http://www.rottentomatoes.com}}. Each snippet is roughly equivalent to a single sentence and includes a positive/negative label, with the entire dataset containing 5331 positive and 5331 negative labelled snippets. For all experiments we randomly selected $\sim 70\%$ of the original dataset as a training set, with the remaining $\sim 30\%$ used as a testing set (see Table \ref{tab:datasets}). In splitting the dataset we have taken care to prevent any snippets from existing in both sets, so as to not contaminate the results.

\begin{table}[t]
    \centering

    \caption{Number of total snippets ($N_{total}$), positive snippets ($N_{pos}$), and negative snippets ($N_{neg}$) for the original, training, and testing datasets.} 
    \label{tab:datasets}

    \ra{1.2}
    \begin{tabular}{@{} l r r r  @{}}
        \\
        \toprule
        \bf{Dataset} & $N_{total}$ & $N_{pos}$ & $N_{neg}$ \\
        \midrule
        Original & 10662 & 5331 & 5331 \\
        Training &  7462 & 0000 & 0000 \\
        Testing  &  3200 & 0000 & 0000 \\
        \bottomrule
    \end{tabular}
\end{table}


%
% OPTIMIZATION
%
\subsection{Optimization}
L-BFGS

\subsubsection{Convergence}
error less than $10^{-6}$ (as stated in the project description)


%
% RAE
%
\subsection{Experiment 1: RAE}
The full method (RAE)


%
% NO DERIVATIVES
%
\subsection{Experiment 2: RAE without Derivatives}
RAE without derivatives to adjust the meaning vector of each word


%
% BAG-OF-WORDS
%
\subsection{Experiment 3: Bag-of-Words}



%-----------------------------------------------------------------------------
% CONCLUSION
%-----------------------------------------------------------------------------
\section{Conclusion}
Final remarks



%-----------------------------------------------------------------------------
% BIBLIOGRAPHY
%-----------------------------------------------------------------------------

\small{
\bibliographystyle{IEEEtran}
\bibliography{sources}
}



\end{document}
